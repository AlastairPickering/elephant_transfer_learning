{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b7105e-acb9-4931-8364-30c0e9e5585a",
   "metadata": {},
   "source": [
    "# Forest elephant vocalisation feature extraction\n",
    "\n",
    "This Jupyter notebook provides a step-by-step guide to using pre-trained CNNs via transfer learning techniques to extract acoustic features from forest elephant vocalisation. In this notebook, we take a dataset containing 1254 different forest elephant vocalisations and automatically extract acoustic features using four different pre-trained CNN models.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "We will be working with audio files of African forest elephants recorded by the Elephant Listening Project in the Dzanga-Bai clearing in the the southwestern Central African Republic between September 2018 and April 2019. The vocalisation dataset has 1254 rows each representing an elephant vocalisation with the start time, end time, low frequency and high frequency annotated alongside the call-type (roar, rumble or trumpet).\n",
    "\n",
    "## Steps\n",
    "1. **Set-up**: Import the libraries and functions needed to conduct the analysis, load the dataset and understand its structure\n",
    "2. **Audio pre-processing**: Pre-process the data to isolate the vocalisations\n",
    "3. **Feature extraction**: Automatically extract the acoustic features using 4 pre-trained Convolutional Neural Networks.\n",
    "4. **Save features**: Save the features for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688eac3-015f-471f-a820-4db57176eab0",
   "metadata": {},
   "source": [
    "### 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27dd090-ea4b-4a41-9cc6-dbebec7cce77",
   "metadata": {},
   "source": [
    "Here we import some pre-defined helper functions located in the `elephants_scripts` folder of the main repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e99adbc-17b3-4acd-ba20-1a8bbd9d530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from elephant_scripts.load_data import load_vocalisation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8d204-bafd-4a38-95cd-1837745a76ff",
   "metadata": {},
   "source": [
    "Now we load the table containing information about each of the elephant vocalisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc569d-4e0b-4952-9931-93ef3241e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = Path(\"audio_dir\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUTPUTS_DIR = Path(\"outputs\")\n",
    "\n",
    "# This function will load the table containing information about each vocalisation\n",
    "# and finds the corresponding audio file in which they appear.\n",
    "df = load_vocalisation_dataset(\n",
    "    DATA_DIR / \"elephant_vocalisations.csv\",\n",
    "    audio_dir=AUDIO_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce4653-14dd-4091-8305-683a15b6a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ba31b-f1bd-4ae3-be11-248a4e6a2ae4",
   "metadata": {},
   "source": [
    "### 2. Audio pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d48595f-e12b-4a00-be60-0f3d32f493b5",
   "metadata": {},
   "source": [
    "Now that the dataframe is loaded and associated with the audio files, we need to pre-process the audio files to extract the exact frequency ranges and time periods associated with the vocalisations. This helps to minimise unwanted environmental sound that may cause erroneous results. These are the steps take to pre-process the files and extract their audio features:\n",
    "\n",
    "<ol type=\"A\">\n",
    "\n",
    "<li>Read the entire audio file.<li>\n",
    "Apply a bandpass filter to exclude frequencies outside the vocalisation range using the low_frequency and high_frequency information.<li>\n",
    "Extract the audio clip corresponding to the vocalisation using the start_time and end_time information.<li>\n",
    "Zero-pad the vocalisation to a length that is a multiple of the input windows of the CNNs and centre the recording within this padding.<li>\n",
    "Normalise the audio clip to have a peak amplitude of 1 to control for elephant distance from the microphone.<li>\n",
    "Reapply the bandpass filter to remove any acoustic artifacts that have been introduced by the pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2fca97-809c-442e-b917-bea2309bf1c9",
   "metadata": {},
   "source": [
    "To visualise how this audio pre-processing works, we can take a sample file and show the effect of each step in the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00608839-755b-4085-ae83-71c9c0ef7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random vocalisation\n",
    "sample = df.sample(n=1).iloc[0]\n",
    "\n",
    "# Print information about the audio file\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b20e0-6db6-479c-954c-994236077e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the steps is implemented as a re-usable function defined in the\n",
    "# feature_extraction.py module within the `elephant_scripts` folder.\n",
    "from elephant_scripts.feature_extraction import (\n",
    "    apply_bandpass_filter,\n",
    "    extract_audio,\n",
    "    generate_spectrogram,\n",
    "    normalise_sound_file,\n",
    "    plot_spectrograms,\n",
    "    read_audio,\n",
    "    zero_pad,\n",
    ")\n",
    "\n",
    "# Set audio parameters used in the functions below\n",
    "DEFAULT_SAMPLERATE = 4000  # Hz\n",
    "DEFAULT_WINDOW_SIZE = 4  # seconds\n",
    "DEFAULT_CLIP_POSITION = \"middle\"  # Default position of the clip within the zero padding\n",
    "\n",
    "# Work through each of the pre-processing steps in turn\n",
    "\n",
    "# 1) Read audio\n",
    "wav, sr = read_audio(sample.audio_filepaths)\n",
    "\n",
    "# 2) Apply bandpass filter\n",
    "filtered_wav = apply_bandpass_filter(\n",
    "    wav,\n",
    "    sample.low_freq,\n",
    "    sample.high_freq,\n",
    "    samplerate=DEFAULT_SAMPLERATE,\n",
    ")\n",
    "\n",
    "# 3) Extract audio segment based on annotation times\n",
    "extracted_audio = extract_audio(filtered_wav, sample, samplerate=DEFAULT_SAMPLERATE)\n",
    "\n",
    "# 4) Zero-pad the wav array\n",
    "padded_wav = zero_pad(\n",
    "    extracted_audio,\n",
    "    annotation=sample,\n",
    "    window_size=DEFAULT_WINDOW_SIZE,\n",
    "    samplerate=DEFAULT_SAMPLERATE,\n",
    "    position=DEFAULT_CLIP_POSITION,\n",
    ")\n",
    "\n",
    "# 5) Normalise the sound file\n",
    "normalised_clip = normalise_sound_file(padded_wav)\n",
    "\n",
    "# 6) Re-apply bandpass filter to the normalised waveform\n",
    "refiltered_wav = apply_bandpass_filter(\n",
    "    normalised_clip,\n",
    "    sample.low_freq,\n",
    "    sample.high_freq,\n",
    "    samplerate=DEFAULT_SAMPLERATE,\n",
    ")\n",
    "\n",
    "# Plot spectrograms for each pre-processing step\n",
    "steps = [\n",
    "    \"1) Read Audio\",\n",
    "    \"2) Apply Bandpass Filter\",\n",
    "    \"3) Extract Audio\",\n",
    "    \"4) Zero-Pad\",\n",
    "    \"5) Normalise Amplitude\",\n",
    "    \"6) Reapply Bandpass filter\",\n",
    "]\n",
    "audios = [\n",
    "    wav,\n",
    "    filtered_wav,\n",
    "    extracted_audio,\n",
    "    padded_wav,\n",
    "    normalised_clip,\n",
    "    refiltered_wav,\n",
    "]\n",
    "plot_spectrograms(\n",
    "    steps,\n",
    "    audios,\n",
    "    sr=DEFAULT_SAMPLERATE,\n",
    "    window_size=DEFAULT_WINDOW_SIZE,\n",
    "    position=DEFAULT_CLIP_POSITION,\n",
    "    figsize=(10, 10),\n",
    "    fontsize=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daabf64-5483-46c4-a686-46e06f38268f",
   "metadata": {},
   "source": [
    "These individual steps are combined in one single function called the wav_cookiecutter which we'll use for the pre-processing the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6ac71-92c6-4cb6-b440-af6ffcb4bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant_scripts.feature_extraction import plot_spectrogram, wav_cookiecutter\n",
    "\n",
    "# The wav_cookiecutter function integrates all the previous step into a single\n",
    "# function.\n",
    "# Note that you can control the amount of zero-padding by changing the window_size\n",
    "# This facilitates adjusting the size of the audio samples to the input sizes required\n",
    "# by the different models.\n",
    "preprocessed = wav_cookiecutter(\n",
    "    sample,\n",
    "    samplerate=DEFAULT_SAMPLERATE,\n",
    "    window_size=DEFAULT_WINDOW_SIZE,\n",
    "    position=DEFAULT_CLIP_POSITION,\n",
    ")\n",
    "\n",
    "plot_spectrogram(preprocessed, figsize=(4, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb18357-7bcd-491f-948c-35e39868683c",
   "metadata": {},
   "source": [
    "We can now apply the `wav_cookiecutter` to all of the vocalisations to see how it affects the 4 CNNs differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8e81f-df0a-4861-9215-415dc0c52de9",
   "metadata": {},
   "source": [
    "**Generate and visualise spectrograms for each CNN after pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b29bbf0-2ca1-4c7b-91f0-5d147b9cdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import all the available models\n",
    "from elephant_scripts.models import BirdNET, Perch, VGGish, YAMNet\n",
    "\n",
    "\n",
    "MODELS = [VGGish, YAMNet, BirdNET, Perch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe779f7d-18d9-4875-8d72-d4f546d2ee73",
   "metadata": {},
   "source": [
    "As each model has specific input requirements, audio must be padded to fit the necessary input size during feature extraction. \n",
    "This padding ensures the vocalisation remains centered within the input audio. \n",
    "Notably, for models like BirdNET and Perch, which require larger input sizes, the elephant vocalisation occupies only a small portion of the total input audio. \n",
    "This can be seen in the following visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27258e13-74f4-4b78-93f3-9f02c003d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant_scripts.plotting import plot_spectrogram_matrix\n",
    "\n",
    "for model in MODELS:\n",
    "    # Adjust the window size to account for the model's native samplerate.\n",
    "    window_size = model.window_size * model.samplerate / DEFAULT_SAMPLERATE\n",
    "\n",
    "    # Plotting spectrograms for each model\n",
    "    plot_spectrogram_matrix(\n",
    "        df,\n",
    "        window_size=window_size,\n",
    "        title=f\"{model.__name__} inputs\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de706c2-bbd8-49d7-ab6b-177ce407f3d5",
   "metadata": {},
   "source": [
    "### 3. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052db072-5301-4472-909c-8af07d0447a1",
   "metadata": {},
   "source": [
    "The pre-processed audio files are then passed through the pre-trained CNNs in their time window multiples and their acoustic features are automatically extracted to produce embeddings. These embeddings are then averaged to obtain a single embedding per vocalisation. This feature extraction phase involves the following steps:\n",
    "\n",
    "1. Extract the acoustic feature embeddings for each sample chunk of the audio clip using the pretrained CNNs.\n",
    "2. Average the embeddings across the chunks to obtain a single embedding for each vocalisation.\n",
    "3. Add in information about the duration of each vocalisation\n",
    "\n",
    "This resulting embedding encodes the acoustic feature representation of the vocalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f40ab2-869f-4ead-82cf-2c6be320fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_features(\n",
    "    annotation,\n",
    "    model,\n",
    "    position: str = DEFAULT_CLIP_POSITION,\n",
    "    samplerate: int = DEFAULT_SAMPLERATE,\n",
    "):\n",
    "    \"\"\"Extract features of a single vocalisation using specified model.\"\"\"\n",
    "    # Preprocess vocalisation audio\n",
    "    wav = wav_cookiecutter(\n",
    "        annotation,\n",
    "        window_size=model.window_size * model.samplerate / DEFAULT_SAMPLERATE,\n",
    "    )\n",
    "\n",
    "    # Compute features using model\n",
    "    features = model.extract_features(wav)\n",
    "\n",
    "    # Average features in case the audio was split into multiple chunks\n",
    "    mean_features = features.mean(axis=0)\n",
    "\n",
    "    # Return features with duration added.\n",
    "    return {\n",
    "        **{f\"feature_{i}\": value for i, value in enumerate(mean_features)},\n",
    "        \"duration\": annotation.duration,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_all_features(\n",
    "    df,\n",
    "    model_type,\n",
    "    output: Path | str | None = None,\n",
    "    position: str = DEFAULT_CLIP_POSITION,\n",
    "    samplerate: int = DEFAULT_SAMPLERATE,\n",
    "    force: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Extract all vocalisation features from the given dataset with the specified model.\"\"\"\n",
    "    \n",
    "    # Instatiate the model. This loads the model weights and prepares it for processing.\n",
    "    model = model_type()\n",
    "\n",
    "    # Check if features have been pre-computed and load if so\n",
    "    if output is not None and Path(output).is_file() and not force:\n",
    "        print(\n",
    "            f\"Pre-processed features for {type(model).__name__} were found, \"\n",
    "            \"skipping computation. To recompute features, use `force=True`.\"\n",
    "        )\n",
    "        return pd.read_parquet(output)\n",
    "\n",
    "    # Compute features for all annotated vocalisations\n",
    "    features = pd.DataFrame(\n",
    "        [\n",
    "            extract_features(annotation, model)\n",
    "            for annotation in tqdm(df.itertuples(), total=len(df))\n",
    "        ],\n",
    "        index=df[\"vocalisation_id\"],\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    if output is not None:\n",
    "        features.to_parquet(output)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98dc0c-1011-4561-ab36-095591836918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the whole vocalisation dataset with all available model and save the output\n",
    "for model_type in MODELS:\n",
    "    model_name = model_type.__name__.lower()\n",
    "    extract_all_features(\n",
    "        df,\n",
    "        model_type,\n",
    "        output=OUTPUTS_DIR / f\"{model_name}_vocalisation_features.parquet\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
