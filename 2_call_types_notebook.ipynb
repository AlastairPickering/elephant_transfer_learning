{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c385f6",
   "metadata": {},
   "source": [
    "# Forest elephant vocalisation call-type classification\n",
    "\n",
    "This Jupyter notebook provides a step-by-step guide to using pre-trained CNNs via transfer learning techniques to classify forest elephant vocalisation call-types and evaluate the performance of these techniques. \n",
    "This notebook evaluates the performance of automated feature extraction methods in distinguishing between three call types from a dataset of 1254 forest elephant vocalisations.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset contains information on African forest elephant vocalisations recorded in Dzanga-Bai clearing, Central African Republic, between September 2018 and April 2019 by the Elephant Listening Project.\n",
    "\n",
    "It includes:\n",
    "\n",
    "1. `elephant_vocalisations.csv`: A table of 1254 annotated vocalisations, each with start time, end time, frequency range, call type (roar, rumble, or trumpet), and corresponding audio file.\n",
    "2. `{model}_vocalisations_features.parquet`: Parquet files storing acoustic features extracted from the vocalisations using the workflow described in the `1_feature_extraction_notebook`. Features are extracted using four different CNN models (VGGish, YAMNet, BirdNET, and Perch).\n",
    "\n",
    "## Steps\n",
    "1. **Set-up**: Load the vocalisations data and the pre-computed features.\n",
    "2. **Dimensionality reduction**: Project the acoustic features into 2D space as the basis for clustering and statistical analysis\n",
    "3. **Silhouette analysis**: Calculate the silhouette scores for the UMAP acoustic feature embeddings.\n",
    "4. **Call-Type Classification**: Train a random forest classifier and assess its classification performance.\n",
    "\n",
    "**Note:** Feature extraction is a computationally intensive process. \n",
    "To avoid re-computation, this notebook uses pre-computed features. \n",
    "Refer to the `1_feature_extraction_notebook` for details on the feature extraction workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c167756",
   "metadata": {},
   "source": [
    "### 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74279d41",
   "metadata": {},
   "source": [
    "Here we will import all dependencies as well as some pre-defined helper functions located in the `elephants_scripts` folder of the main repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from elephant_scripts.load_data import load_vocalisation_dataset\n",
    "\n",
    "AUDIO_DIR = Path(\"audio_dir\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUTPUTS_DIR = Path(\"outputs\")\n",
    "\n",
    "# Now we load the table containing information about each of the elephant vocalisations.\n",
    "df = load_vocalisation_dataset(\n",
    "    DATA_DIR / \"elephant_vocalisations.csv\",\n",
    "    audio_dir=AUDIO_DIR,\n",
    ")\n",
    "\n",
    "# And we load each of the pre-computed features.\n",
    "MODELS = [\"VGGish\", \"YAMNet\", \"BirdNET\", \"Perch\"]\n",
    "\n",
    "features = {\n",
    "    model: pd.read_parquet(\n",
    "        OUTPUTS_DIR / f\"{model.lower()}_vocalisation_features.parquet\"\n",
    "    )\n",
    "    for model in MODELS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba778b0",
   "metadata": {},
   "source": [
    "### 2. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97492d2d",
   "metadata": {},
   "source": [
    "Now that we have the feature embeddings for each of the 1254 recordings we need to reduce this high-dimensional data into lower dimensional space to make it interpretable to the human brain and usable in the statistical tests. This involves 2 steps:\n",
    "1. Normalise the embeddings so that their mean = 0 and variance = 1. This ensures equal weighting of the features\n",
    "2. Carry out the dimensionality reduction with specified parameters, including the number of components (2) and distance metric we want to use (cosine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f2f3b",
   "metadata": {},
   "source": [
    "**Normalisation step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7cddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Function to normalise the DataFrame\n",
    "def normalise_features(features):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform the features\n",
    "    normalised_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Create a new DataFrame for the normalised features\n",
    "    normalised = pd.DataFrame(\n",
    "        normalised_features,\n",
    "        columns=features.columns,\n",
    "        index=features.index,\n",
    "    )\n",
    "\n",
    "    # Return the normalised DataFrame\n",
    "    return normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7111cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise each of the extracted features\n",
    "normalised = {model: normalise_features(feats) for model, feats in features.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a6d44",
   "metadata": {},
   "source": [
    "**Dimensionality reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597a35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "# Specify the UMAP parameters\n",
    "N_COMP = 2  # select 1, 2 or 3 dimensions\n",
    "METRIC = \"cosine\"  # distance metric used\n",
    "N_NEIGHBORS = 15\n",
    "MIN_DIST = 0\n",
    "RANDOM_STATE = 2204\n",
    "\n",
    "\n",
    "# Function to fit UMAP and merge metadata\n",
    "def process_umap(\n",
    "    normalised_df,\n",
    "    metadata_df,\n",
    "    n_comp=N_COMP,\n",
    "    metric=METRIC,\n",
    "    min_dist=MIN_DIST,\n",
    "    n_neighbors=N_NEIGHBORS,\n",
    "    random_state=RANDOM_STATE,\n",
    "):\n",
    "    # Instantiate UMAP projector with provided parameters\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=N_COMP,\n",
    "        metric=metric,\n",
    "        min_dist=min_dist,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Fit UMAP and obtain embeddings\n",
    "    embedding = reducer.fit_transform(normalised_df)\n",
    "\n",
    "    # Create DataFrame with UMAP embeddings, preserving 'vocalisation_id' as index\n",
    "    umap_results = pd.DataFrame(\n",
    "        embedding,\n",
    "        columns=[f\"UMAP{i + 1}\" for i in range(N_COMP)],\n",
    "        index=normalised_df.index,\n",
    "    )\n",
    "\n",
    "    # Merge UMAP coordinates with metadata to obtain the\n",
    "    # corresponding call type\n",
    "    return umap_results.merge(metadata_df, on=\"vocalisation_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70291af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "umaps = {model: process_umap(norm, df) for model, norm in normalised.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e2815",
   "metadata": {},
   "source": [
    "### 3. Silhouette analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51222a4e-c688-4677-a492-c62a1559f0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T16:55:03.513461Z",
     "iopub.status.busy": "2025-01-15T16:55:03.513242Z",
     "iopub.status.idle": "2025-01-15T16:55:03.519016Z",
     "shell.execute_reply": "2025-01-15T16:55:03.518203Z",
     "shell.execute_reply.started": "2025-01-15T16:55:03.513444Z"
    }
   },
   "source": [
    "To evaluate the effectiveness of our feature extraction method in grouping different call types, we use the silhouette score. This score measures how similar each data point is to its own cluster compared to other clusters. It is calculated by comparing the average distance to all points within the same cluster against the average distance to points in the nearest neighboring cluster (for a detailed explanation, see [silhouette-coefficient](https://scikit-learn.org/1.5/modules/clustering.html#silhouette-coefficient))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b0be0c-6504-4d78-9f2f-0a85aafb1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "def silhouette_report(umap_df, groupby=\"call_type\"):\n",
    "    labels = umap_df[groupby]\n",
    "    silhouette_avg = silhouette_score(umap_df[[\"UMAP1\", \"UMAP2\"]], labels)\n",
    "    silhouette_values = silhouette_samples(umap_df[[\"UMAP1\", \"UMAP2\"]], labels)\n",
    "\n",
    "    # Prepare a dictionary to store average silhouette scores for each label\n",
    "    silhouette_dict = {\n",
    "        \"Average\": silhouette_avg,\n",
    "    }\n",
    "    unique_labels = labels.unique()\n",
    "    for label in unique_labels:\n",
    "        label_indices = labels == label\n",
    "        avg_silhouette_score = silhouette_values[label_indices].mean()\n",
    "        silhouette_dict[label] = avg_silhouette_score\n",
    "\n",
    "    return silhouette_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e94990-58fd-4648-8d66-bb252d473d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = pd.DataFrame(\n",
    "    {\n",
    "        model: silhouette_report(projected)\n",
    "        for model, projected in umaps.items()\n",
    "    }\n",
    ")\n",
    "silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450da737-a6f6-4586-a1a4-27f31a035c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T16:49:51.293591Z",
     "iopub.status.busy": "2025-01-15T16:49:51.293022Z",
     "iopub.status.idle": "2025-01-15T16:49:51.303766Z",
     "shell.execute_reply": "2025-01-15T16:49:51.302168Z",
     "shell.execute_reply.started": "2025-01-15T16:49:51.293556Z"
    }
   },
   "source": [
    "**Plot UMAP 2D for each model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from elephant_scripts.plotting import plot_umap_with_silhouette\n",
    "\n",
    "# Plotting all 4 models' UMAPs in a 2x2 grid (scaling appropriately for the grid)\n",
    "fig, axs = plt.subplots(\n",
    "    2, 2, figsize=(20, 20), dpi=300\n",
    ")  # Set a good DPI for high quality\n",
    "\n",
    "# Plot each model's UMAP with scaling\n",
    "plot_umap_with_silhouette(umaps[\"VGGish\"], \"VGGish\", axs[0, 0])\n",
    "plot_umap_with_silhouette(umaps[\"Perch\"], \"Perch\", axs[0, 1])\n",
    "plot_umap_with_silhouette(umaps[\"YAMNet\"], \"YAMNet\", axs[1, 0])\n",
    "plot_umap_with_silhouette(umaps[\"BirdNET\"], \"BirdNET\", axs[1, 1])\n",
    "\n",
    "# Adjust layout for a clean 2x2 grid\n",
    "plt.tight_layout(pad=4.0)  # Ensure no overlap, adjust spacing\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29892db3",
   "metadata": {},
   "source": [
    "**Add spectrogram images to UMAP 2D plot to visualise separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a79646-ad91-4a62-a978-414425ea0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant_scripts.plotting import scatter_spec\n",
    "\n",
    "fig = scatter_spec(\n",
    "    umaps[\"BirdNET\"],\n",
    "    column_size=8,\n",
    "    matshow_kwargs={\"cmap\": plt.cm.magma},\n",
    "    scatter_kwargs={\n",
    "        \"alpha\": 0.75,\n",
    "        \"s\": 40,\n",
    "    },\n",
    "    line_kwargs={\"lw\": 1, \"ls\": \"dashed\", \"alpha\": 0.5},\n",
    "    draw_lines=True,\n",
    "    figsize=(20, 20),\n",
    "    range_pad=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0489eb3",
   "metadata": {},
   "source": [
    "### 4. Call-Type Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6ad3b",
   "metadata": {},
   "source": [
    "Lastly, we use the UMAP acoustic feature embeddings to train a Random Forest Classifier to predict the 3 Call-Types on unseen test data. We perform hyperparameter optimisation on the Random Forest model and cross validation on the resultant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da26964a-c911-485c-ba12-8e0befe4de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "\n",
    "# Define a function to handle the model training and evaluation for each category\n",
    "def train_evaluate_category(X, y, category_name):\n",
    "    # Define outer cross-validation strategy\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize list to store results\n",
    "    best_accuracy_list = []\n",
    "\n",
    "    # Loop through outer cross-validation folds\n",
    "    for train_index, test_index in outer_cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Oversample the minority class using RandomOverSampler with automatic sampling strategy\n",
    "        ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=42)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Hyperparameter optimization using GridSearchCV (Random Forest parameters)\n",
    "        param_grid = {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "            \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "        }\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(rf, param_grid, cv=3, scoring=\"accuracy\", verbose=1)\n",
    "        grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "        # Store best hyperparameters and accuracy\n",
    "        print(f\"Best Parameters for {category_name}:\", grid_search.best_params_)\n",
    "        print(f\"Best Accuracy for {category_name}:\", grid_search.best_score_)\n",
    "\n",
    "        # Get the best model\n",
    "        rf_best = grid_search.best_estimator_\n",
    "\n",
    "        # Evaluate the model using the outer fold test data\n",
    "        y_pred_best = rf_best.predict(X_test)\n",
    "\n",
    "        # Calculate macro average accuracy score for classification\n",
    "        accuracy = (\n",
    "            grid_search.best_score_\n",
    "        )  # Using best score from grid search (inner CV)\n",
    "        best_accuracy_list.append(accuracy)\n",
    "\n",
    "        # Print classification report for the best model\n",
    "        print(\n",
    "            f\"Random Forest {category_name} Classification Report:\\n\",\n",
    "            classification_report(y_test, y_pred_best),\n",
    "        )\n",
    "\n",
    "    # Return average of best accuracy across folds\n",
    "    return np.mean(best_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61794983-76f5-4f7d-8007-21f26a662fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Initialize the results dictionary\n",
    "results = {}\n",
    "\n",
    "# Run model training and evaluation for each model/umap_df\n",
    "for model_name, umap_df in umaps.items():\n",
    "    X = umap_df[[\"UMAP1\", \"UMAP2\"]]  # UMAP features\n",
    "    y = umap_df[\"call_type\"]  # Target labels\n",
    "    mean_accuracy = train_evaluate_category(X, y, model_name)\n",
    "    results[model_name] = mean_accuracy\n",
    "    print(f\"Mean accuracy for {model_name}: {mean_accuracy}\")\n",
    "\n",
    "# Store and present the results in a table\n",
    "results_df = pd.DataFrame(\n",
    "    list(results.items()), columns=[\"Model\", \"Best Macro Average Accuracy\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
